# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml#L32
seed: 42

n_timesteps: !!float 3e8 
policy: 'MultiInputPolicy'
#n_steps: 64 # this is n_step per env, larger num_env with smaller n_steps
batch_size: 128 # make it smaller than n_steps * num_env
#gae_lambda: 0.95
gamma: 0.99
#n_epochs: 10 #20
#ent_coef: 0. #0.01
learning_rate: !!float 3e-4
#clip_range: !!float 0.2
policy_kwargs: "dict(
    features_extractor_class=CustomCombinedExtractor
)"
#policy_kwargs: "dict(
#                  activation_fn=nn.ELU,
#                  net_arch=[32, 32],
#                  squash_output=False,
#                )"
#vf_coef: 0.5 #1.0
#max_grad_norm: 0.5 #1.0
buffer_size: 100000  # Add this for replay buffer
tau: 0.005                  # Polyak averaging parameter for target network

device: "cuda:0"
